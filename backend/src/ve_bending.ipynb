{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "6Rk2n26oLGkT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load original dataset\n",
        "df = pd.read_csv(\"bending_machine_data.csv\")  # Replace with actual dataset path\n",
        "target_col = \"result\"  # Define the target column"
      ],
      "metadata": {
        "id": "Vaq7fCMqQgfB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "X = df.drop(columns=[target_col]).values\n",
        "y = df[target_col].values\n",
        "input_dim = X.shape[1]"
      ],
      "metadata": {
        "id": "pf52yJ_lQgjh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Variational Autoencoder (VAE)\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mu_layer = nn.Linear(64, latent_dim)\n",
        "        self.logvar_layer = nn.Linear(64, latent_dim)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.encoder(x)\n",
        "        mu, logvar = self.mu_layer(hidden), self.logvar_layer(hidden)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n"
      ],
      "metadata": {
        "id": "PdExBPUPQgmx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize VAE\n",
        "latent_dim = 16\n",
        "vae = VAE(input_dim, latent_dim)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    recon_loss = nn.MSELoss()(recon_x, x)\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kl_divergence"
      ],
      "metadata": {
        "id": "Kt4lH-9-Qgqg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train VAE\n",
        "def train_vae(epochs=500, batch_size=32):\n",
        "    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for real_data in dataloader:\n",
        "            real_data = real_data[0]\n",
        "            optimizer.zero_grad()\n",
        "            recon_data, mu, logvar = vae(real_data)\n",
        "            loss = loss_function(recon_data, real_data, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")\n",
        "\n",
        "train_vae()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EKpkypXQgtQ",
        "outputId": "ee7e4ebc-be86-4fa2-f2f0-92704bf1f7b6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 1851.6326\n",
            "Epoch 50: Loss 124.7452\n",
            "Epoch 100: Loss 75.2751\n",
            "Epoch 150: Loss 60.3964\n",
            "Epoch 200: Loss 51.3613\n",
            "Epoch 250: Loss 70.6211\n",
            "Epoch 300: Loss 51.9873\n",
            "Epoch 350: Loss 51.6538\n",
            "Epoch 400: Loss 38.7268\n",
            "Epoch 450: Loss 49.7153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "z = torch.randn(500, latent_dim)\n",
        "synthetic_data = vae.decoder(z).detach().numpy()"
      ],
      "metadata": {
        "id": "C6JEWmqKQgvq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save synthetic data\n",
        "synthetic_df = pd.DataFrame(synthetic_data, columns=df.drop(columns=[target_col]).columns)\n",
        "synthetic_df[target_col] = np.random.uniform(y.min(), y.max(), size=synthetic_df.shape[0])\n",
        "synthetic_df.to_csv(\"ve_synthetic_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "279LHFTbQ8pp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create augmented dataset\n",
        "augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
        "augmented_df.to_csv(\"ve_augmented_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "70vvUFqGQ8tR"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}